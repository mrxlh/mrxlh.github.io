##  电商场景问题解决方案：

#### 1. 如何避免重复下单

​        前端页面上应该防止用户重复提交表单，你说的没错。但是，网络错误会导致重传，很多 RPC 框架、网关都会有自动重试机制，所以对于订单服务来说，重复请求这 个事儿，你是没办法完全避免的,

解决办法是，让你的订单服务具备幂等性。

​       具体的做法是这样的，我们给订单系统增加一个“生成订单号”的服务，这个服务没有参 数，返回值就是一个新的、全局唯一的订单号。在用户进入创建订单的页面时，前端页面先 调用这个生成订单号服务得到一个订单号，在用户提交订单的时候，在创建订单的请求中带 着这个订单号。

#### 2. 如何解决 ABA 问题？

​        正常情况下，订单中的快递单号会先更新成 666，再更新成 888，这是没问题的。那不正 常情况呢？666 请求到了，单号更新成 666，然后 888 请求到了，单号又更新成 888，但 是 666 更新成功的响应丢了，调用方没收到成功响应，自动重试，再次发起 666 请求，单 号又被更新成 666 了，这数据显然就错了。这就是非常有名的 ABA 问题。

​        在订单主表增加一列， 列名可以叫 version，也即是“版本号”的意思。每次查询订单的时候，版本号需要随着订 单数据返回给页面。页面在更新数据的请求中，需要把这个版本号作为更新请求的参数，再 带回给订单更新服务。

~~~sql
UPDATE orders set tracking_number = 666, version = version + 1
WHERE version = 8;
~~~

在这条 SQL 的 WHERE 条件中，version 的值需要页面在更新的时候通过请求传进来。



#### 3. 电商系统中如何解决超卖这个问题

​       一般来说，解决超卖问题的方法都是在下单的时候锁定库存，如果订单取消再释放库存。配合发货的时候减库存，才不会超卖。

#### 4. 流量大、数据多的商品详情页系统该如何设计

​       一定要记得保留商品数据的每一个历 史版本。因为商品数据是随时变化的，但是订单中关联的商品数据，必须是下单那个时刻的 商品数据，这一点很重要。你可以为每一个历史版本的商品数据保存一个快照，可以创建一 个历史表保存到 MySQL 中，也可以保存到一些 KV 存储中。

​       使用 MongoDB 保存商品参数

MongoDB 是一个面向文档存储的 NoSQL 数据库，在 MongoDB 中，表、行、列对应的 概念分别是：collection、document、field，

​       MongoDB 中的每一行数据，在存储层就是简单地被转化成 BSON 格 式后存起来，这个 BSON 就是一种更紧凑的 JSON。所以，即使在同一张表里面，它每一 行数据的结构都可以是不一样的。当然，这样的灵活性也是有代价的，MongoDB 不支持 SQL，多表联查和复杂事务比较孱弱，不太适合存储一般的数据。 但是，对于商品参数信息，数据量大、数据结构不统一，这些 MongoDB 都可以很好的满 足。我们也不需要事务和多表联查，MongoDB 简直就是为了保存商品参数量身定制的一 样。

  使用对象存储保存图片和视频

​        图片和视频由于占用存储空间比较大，一般的存储方式都是，在数据库中只保存图片视频的 ID 或者 URL，实际的图片视频以文件的方式单独存储。

​      访问图片视频的时候，真正的图片和视频文件也不需要经过商品系统的后端服务，页面直接 通过对象存储提供的 URL 来访问，又省事儿又节约带宽。而且，几乎所有的对象存储云服 务都自带 CDN（Content Delivery Network）加速服务，响应时间比直接请求业务的服务 器更短。  国内的很多云厂商的对象存储对图片和视频，都做了非常多的针对性优化。最有用的是，缩 放图片和视频转码，你只要把图片和视频丢到对象存储中，就可以随时获得任意尺寸大小的 图片，视频也会自动转码成各种格式和码率的版本，适配各种 App 和场景。

将商品介绍静态化：

把这个页面事先生成好， 保存成一个静态的 HTML，访问商详页的时候，直接返回这个 HTML。这就是静态化。 商详页静态化之后，不仅仅是可以节省服务器资源，还可以利用 CDN 加速，把商详页放到 离用户最近的 CDN 服务器上，让商详页访问更快。 至于商品价格、促销信息等这些需要频繁变动的信息，不能静态化到页面中，可以在前端页 面使用 AJAX 请求商品系统动态获取。这样就兼顾了静态化带来的优势，也能解决商品价 格等信息需要实时更新的问题。

#### 5. 购物车系统，应该如何设计

​       在用户选购商品时，下单之前，暂存用 户想要购买的商品。购物车对数据可靠性要求不高，性能也没有特别的要求，在整个电商系 统中，看起来是相对比较容易设计和实现的一个子系统。

#### 6. 账户余额总是对不上账，怎么办

“对不上账”是通俗的说法，它的本质问题是，冗余数据的一致性问题。

流水和余额也是互为冗余数据，但是记录流水，可以有效地修正由于系统 Bug 或 者人为篡改导致的账户余额错误的问题，也便于账户系统与其他外部系统进行对账，所以账 户系统记录流水是非常必要的。 在设计账户流水时，有几个重要的原则必须遵守，最好是用技术手段加以限制。 在对账的时候，一旦出现了流水和余额不一致，并且无法通过业务手段来确定到底是哪儿记 错了的情况，一般的处理原则是以交易流水为准来修正余额数据，这样才能保证后续的交易 能“对上账”。 

​        那从技术上，如何保证账户系统中流水和余额数据一致呢？ 使用数据库事务来保证数据一致性 在设计对外提供的服务接口时，不能提供单独更新余额或者流水的功能，只提供交易功能。 我们需要在实现交易功能的时候，同时记录流水并修改余额，并且要尽可能保证，在任何情 流水记录只能新增，一旦记录成功不允许修改和删除。即使是由于正当原因需要取消一 笔已经完成的交易，也不应该去删除交易流水。正确的做法是再记录一笔“取消交 易”的流水。

​     1.流水记录只能新增，一旦记录成功不允许修改和删除。即使是由于正当原因需要取消一 笔已经完成的交易，也不应该去删除交易流水。正确的做法是再记录一笔“取消交 易”的流水。 

2.   流水号必须是递增的，我们需要用流水号来确定交易的先后顺序。

   

   要么都成功，要么都失败，即使是在 数据库宕机、应用程序退出等等这些异常情况下，也不会出现，只更新了一个表而另一个表 没更新的情况。这是事务的原子性（Atomic）

   

   数据库为了实现一致性，必须保证每个事务的执行过程中，中间状态对其他事务是不可见 的。这是事务的隔离性 (Isolation)

   最后，只要事务提交成功，数据一定会被持久化到磁盘中，后续即使发生数据库宕机，也不 会改变事务的结果。这是事务的持久性 (Durability)。

   在一个事务执行过程中，它能不能读到其他已提交事务对数据的更新，如果能读到数据变 化，就是“不可重复读”，否则就是“可重复读”。

   在同一个事务内两次读取同一条数据，读到 的结果可能会不一样，这就是“不可重复读”。

   在 RR 隔离级别下，在一个事务进行过程中，对于同一条数据，每次读到的结 果总是相同的，无论其他会话是否已经更新了这条数据，这就是“可重复读”。

   

   #### 7. 分布式事务：如何保证多个系统间的数据是一致的？

   ​       在之前大系统的时代，普遍的做法是，在设计时尽量避免这种跨系统跨数据库的交易。 但是，现在的技术趋势是云原生和微服务，微服务它的理念是什么？大系统被打散成多个小 的微服务，每个微服务独立部署并且拥有自己的数据库，大数据库也被打散成多个小的数据 库。跨越微服务和数据库的交易就成为一种越来越普遍的情况。我们的业务系统微服务化之 后，不可避免地要面对跨系统的数据一致性问题。

   

   ​       理论上，分布式事务也是事务，也需要遵从 ACID 四个特性，但实际情况是，在分布式系 统中，因为必须兼顾性能和高可用，所以是不可能完全满足 ACID 的。

   分布式事务的解决方案有很多，比如：2PC、3PC、TCC、Saga 和本地消息表，这里面，2PC 和本地消息表这两 种分布式事务的解决方案，比较贴近于我们日常开发的业务系统。

   ​      只有在需要强一致、并且并发量不大的场景下，才考虑使用 2PC。

   本地消息表非常适合解决这种分布式最终一致性的问题。

    	本地消息表这种方法，它只能满足 D（持久性），A（原子 性）C（一致性）、I（隔离性）都比较差，但是，它的优点非常突出。

   即使能接受数据最终一致，本地消息表也不是什么场景都可以适用的。它有一个前提 条件就是，异步执行的那部分操作，不能有依赖的资源。比如说，我们下单的时候，除了要 清空购物车以外，还需要锁定库存。

#### 7. MySQL HA：如何将“删库跑路”的损失降到最低？

一般来说，存储系统导致的比较严重的损失主要有两种情况，一是数据丢失造成的直接财产 损失，比如大量的坏账；二是由于存储系统损坏，造成整个业务系统停止服务而带来的损 失。

~~~sql
# mysqldump -uroot -p test > test.sql # 全量备份
# mysql -uroot test < test.sql   # 恢复

~~~

备份文件包含数据库中的所有数据，占用的磁盘空间非常大；其次，每次备份操作都 要拷贝大量数据，备份过程中会占用数据库服务器大量的 CPU、磁盘 IO 资源，并且为了 保证数据一致性，还有可能会锁表，这些都会导致备份期间，数据库本身的性能严重下降。 所以，我们不能经常对数据库执行全量备份。

​       MySQL 自带了 Binlog，就是一种实时的增量备份。Binlog 里面记录的就是 MySQL 数据 的变更的操作日志，开启 Binlog 之后，我们对 MySQL 中的每次更新数据操作，都会被记 录到 Binlog 中。

Binlog 是可以回放的，回放 Binlog，就相当于把之前对数据库所有数据更新操作按照顺序 重新执行了一遍，回放完成之后数据自然就恢复了。

~~~sql
show variables like ‘%log_bin%’
log_bin | ON 

show master status; # 命令可查看当前 Binlog 的状态，显示正在写入的 Binlog 文件，及当前的位置


$mysqlbinlog --start-datetime "2020-02-20 00:00:00" --stop-datetime "2020-02-20 15:08:12" # binlog恢复

执行备份和恢复的时候，有几个要点你需要特别的注意。
第一，也是最重要的，“不要把所有的鸡蛋放在同一个篮子中”，无论是全量备份还是
Binlog，都不要和数据库存放在同一个服务器上。最好能做到不同机房，甚至不同城市，
离得越远越好。这样即使出现机房着火、光缆被挖断甚至地震也不怕。
第二，在回放 Binlog 的时候，指定的起始时间可以比全量备份的时间稍微提前一点儿，确
保全量备份之后的所有操作都在恢复的 Binlog 范围内，这样可以保证恢复的数据的完整
性。
因为回放 Binlog 的操作是具备幂等性的（为了确保回放幂等，需要设置 Binlog 的格式为
ROW 格式)
~~~

主从数据库的思路

1. 在主库的磁盘上写入 Binlog； 2. 主库更新存储引擎中的数据； 3. 给客户端返回成功响应； 4. 主库把 Binlog 复制到从库； 5. 从库回放 Binlog，更新存储引擎中的数据。



| 方案                         | 高可用 | 可能丢数据 | 性能 |
| ---------------------------- | ------ | ---------- | ---- |
| 一主一从，异步复制，手动切换 | 否     | 可控       | 好   |
| 一主一从，异步复制，自动切换 | 是     | 是         | 好   |
| 一主两从，异步复制，自动切换 | 是     | 否         | 差   |

高可用 依赖的是数据复制，数据复制的本质就是从一个库备份数据，然后恢复到另外一个库中去。

 #### 8. 访问数据库超时

每一个做电商的公司都梦想着做社交引流，每一个做社交的公司都梦想着做电商将流量变 现。

当数据库非常忙的时候，它执行任何一个 SQL 都很慢。所 以，并不是说，慢 SQL 日志中记录的这些慢 SQL 都是有问题的 SQL。大部分情况下，导 致问题的 SQL 只是其中的一条或者几条。不能简单地依据执行次数和执行时长进行判断， 但是，单次执行时间特别长的 SQL，仍然是应该重点排查的对象。

 第一，在编写 SQL 的时候，一定要小心谨慎地仔细评估。先问自己几个问题： 

你的 SQL 涉及到的表，它的数据规模是多少？ 

你的 SQL 可能会遍历的数据量是多少？ 

尽量地避免写出慢 SQL。 

第二，能不能利用缓存减少数据库查询次数？在使用缓存的时候，还需要特别注意的就是缓 存命中率，要尽量避免请求命中不了缓存，穿透到数据库上。

遍历行数在千万左右，是 MySQL 查询的一个坎儿。MySQL 中单个表数据量，也要尽量控 制在一千万条以下，最多不要超过二三千万这个量级。原因也很好理解，对一个千万级别的 表执行查询，加上几个 WHERE 条件过滤一下，符合条件的数据最多可能在几十万或者百 万量级，这还可以接受。但如果再和其他的表做一个联合查询，遍历的数据量很可能就超过 千万级别了。所以，每个表的数据量最好小于千万级别。

关系型数据库：

几乎所有的数据库，都是由执行器和存储引擎两 部分组成，执行器负责执行计算，存储引擎负责保存数据。

#### 9. 缓存穿透引起雪崩

如果我们的缓存命中率比较低，就会出现大量“缓存穿透”的情况。缓存穿透指的是，在读 数据的时候，没有命中缓存，请求“穿透”了缓存，直接访问后端数据库的情况。 少量的缓存穿透是正常的，我们需要预防的是，短时间内大量的请求无法命中缓存，请求穿 透到数据库，导致数据库繁忙，请求超时。大量的请求超时还会引发更多的重试请求，更多 的重试请求让数据库更加繁忙，这样恶性循环导致系统雪崩。

当系统初始化的时候，比如说系统升级重启或者是缓存刚上线，这个时候缓存是空的，如果 大量的请求直接打过来，很容易引发大量缓存穿透导致雪崩。为了避免这种情况，可以采用 灰度发布的方式，先接入少量请求，再逐步增加系统的请求数量，直到全部请求都切换完 成。

如果系统不能采用灰度发布的方式，那就需要在系统启动的时候对缓存进行预热。所谓的缓 存预热就是在系统初始化阶段，接收外部请求之前，先把最经常访问的数据填充到缓存里 面，这样大量请求打过来的时候，就不会出现大量的缓存穿透了。

还有一种常见的缓存穿透引起雪崩的情况是，当发生缓存穿透时，如果从数据库中读取数据 的时间比较长，也容易引起数据库雪崩。

#### 10.  读写分离

除非系统规模真的大到只有这一条路可以走，不建议你对 数据进行分片，自行构建 MySQL 集群，代价非常大。

一个简单而且非常有效的方案是，我们不对数据分片，而是使用多个具有相同数据的 MySQL 实例来分担大量的查询请求，这种方法通常称为“读写分离”。

分离应用程序的读写 请求方法有下面这三种：

1. 部署一主多从多个 MySQL 实例，并让它们之间保持数据实时同步。 2. 分离应用程序对数据库的读写请求，分别发送给从库和主库。 



分离应用程序的读写 请求方法有下面这三种：

1. 纯手工方式：修改应用程序的 DAO 层代码，定义读写两个数据源，指定每一个数据库 请求的数据源。 

   2.组件方式：也可以使用像 Sharding-JDBC 这种集成在应用中的第三方组件来实现，这些 组件集成在你的应用程序  内，代理应用程序的所有数据库请求，自动把请求路由到对应 数据库实例上。

3. 代理方式：在应用程序和数据库实例之间部署一组数据库代理实例，比如说 Atlas 或者 MaxScale。对应用程序来说，数据库代理把自己伪装成一个单节点的 MySQL 实例，应 用程序的所有数据库请求被发送给代理，代理分离读写请求，然后转发给对应的数据库 实例。

   

   这三种方式，我最推荐的是第二种，使用读写分离组件。这种方式代码侵入非常少，并且兼 顾了性能和稳定性。如果你的应用程序是一个逻辑非常简单的微服务，简单到只有几个 SQL，或者是，你的应用程序使用的编程语言没有合适的读写分离组件，那你也可以考虑使 用第一种纯手工的方式来实现读写分离。

    一般情况下，不推荐使用第三种代理的方式，原因是，使用代理加长了你的系统运行时数据 库请求的调用链路，有一定的性能损失，并且代理服务本身也可能出现故障和性能瓶颈等问 题。但是，代理方式有一个好处是，它对应用程序是完全透明的。所以，只有在不方便修改 应用程序代码这一种情况下，你才需要采用代理方式。

   如果你配置了多个从库，推荐你使用“HAProxy+Keepalived”这对儿经典的组 合，来给所有的从节点做一个高可用负载均衡方案，既可以避免某个从节点宕机导致业务可 用率降低，也方便你后续随时扩容从库的实例数量。因为 HAProxy 可以做 L4 层代理，也 就是说它转发的是 TCP 请求，所以用“HAProxy+Keepalived”代理 MySQL 请求，在部 署和配置上也没什么特殊的地方，正常配置和部署就可以了。

#### 11.读写分离带来的数据不一致问题

读写分离的一个副作用是，可能会存在数据不一致的情况。原因是，数据库中的数据在主库 完成更新后，是异步同步到每个从库上的，这个过程有一个微小的时间差，这个时间差叫主 从同步延迟。正常情况下，主从延迟非常小，不超过 1ms。但即使这个非常小的延迟，也 会导致在某一个时刻，主库和从库上的数据是不一致的。应用程序需要能接受并克服这种主 从不一致的情况，否则就会引发一些由于主从延迟导致的数据错误。

还是拿订单系统来举例，我们自然的设计思路是，用户从购物车里发起结算创建订单，进入 订单页，打开支付页面进行支付，支付完成后，按道理应该再返回支付之前的订单页。但如 果这个时候马上自动返回订单页，就很可能会出现订单状态还是显示“未支付”。因为，支 付完成后，订单库的主库中，订单状态已经被更新了，而订单页查询的从库中，这条订单记 录的状态有可能还没更新。怎么解决？ 这种问题其实没什么好的技术手段来解决，所以你看大的电商，它支付完成后是不会自动跳 回到订单页的，它增加了一个无关紧要的“支付完成”页面，其实这个页面没有任何有效的 信息，就是告诉你支付成功，然后再放一些广告什么的。你如果想再看刚刚支付完成的订 单，需要手动点一下，这样就很好地规避了主从同步延迟的问题。



在异步复制的情况下，为什么主库宕机存在丢数据的风险？为什么读写分离存在读到 脏数据的问题？产生这些问题，都是因为异步复制它没有办法保证数据能第一时间复制到从 库上。

异步复制时，主库提交事务之后，就会给客户端返回响 应；而同步复制时，主库在提交事务的时候，会等待数据复制到所有从库之后，再给客户端 返回响应。 同步复制这种方式在实际项目中，基本上没法用，原因有两个：一是性能很差，因为要复制 到所有节点才返回响应；二是可用性也很差，主库和所有从库任何一个数据库出问题，都会 影响业务。

为了解决这个问题，MySQL 从 5.7 版本开始，增加一种半同步复制（Semisynchronous Replication）的方式。异步复制是，事务线程完全不等复制响应；同步复制是，事务线程 要等待所有的复制响应；半同步复制介于二者之间，事务线程不用等着所有的复制成功响 应，只要一部分复制响应回来之后，就可以给客户端返回了

一主二从的集群，配置成半同步复制，只要数据成功复制到任意一个从库上，主库 的事务线程就直接返回了。这种半同步复制的方式，它兼顾了异步复制和同步复制的优点。 如果主库宕机，至少还有一个从库有最新的数据，不存在丢数据的风险。并且，半同步复制 的性能也还凑合，也能提供高可用保证，从库宕机也不会影响主库提供服务。所以，半同步 复制这种折中的复制方式，也是一种不错的选择。

“rpl_semi_sync_master_wait_no_slave”， 含义是：“至少等待数据复制到几个从节点再返回”。这个数量配置的越大，丢数据的风险 越小，但是集群的性能和可用性就越差。最大可以配置成和从节点的数量一样，这样就变成 了同步复制。

一般情况下，配成默认值 1 也就够了，这样性能损失最小，可用性也很高，只要还有一个 从库活着，就不影响主库读写。丢数据的风险也不大，只有在恰好主库和那个有最新数据的 从库一起坏掉的情况下，才有可能丢数据。

“rpl_semi_sync_master_wait_point“  这个参数控制主库执行事 务的线程，是在提交事务之前（AFTER_SYNC）等待复制，还是在提交事务之后 （AFTER_COMMIT）等待复制。默认是 AFTER_SYNC，也就是先等待复制，再提交事 务，这样完全不会丢数据。AFTER_COMMIT 具有更好的性能，不会长时间锁表，但还是 存在宕机丢数据的风险。

复制状态机：所有分布式存储都是这么复制数据的，“快照 + 操作日志”的方法，不是 MySQL 特有的。

Redis Cluster 中，它的全量备份称为 Snapshot，操作日志叫 backlog，它的主 从复制方式几乎和 MySQL 是一模一样的。

几乎所有的存储 系统和数据库，都是用这一套方法来解决备份恢复和数据复制问题的。

​         这一套方法其实是有理论基础的，叫做复制状态机 (Replication State Machine)，我能查到的最早的出处是 1978 年 Lamport 的一篇论文《The Implementation of Reliable Distributed Multiprocess Systems》。

https://xueshu.baidu.com/usercenter/paper/show?paperid=718b8edac5c49982f78f5f0d6dd2231f&site=xueshu_se

​        复制数据的时候，只要基于一个快照，按照顺序执行快照之后的所有操作日志，就可以得到 一个完全一样的状态。在从节点持续地从主节点上复制操作日志并执行，就可以让从节点上 的状态数据和主节点保持同步。

​       主从同步做数据复制时，一般可以采用几种复制策略。性能最好的方法是异步复制，主节点 上先记录操作日志，再更新状态数据，然后异步把操作日志复制到所有从节点上，并在从节 点执行操作日志，得到和主节点相同的状态数据。 

​       异步复制的劣势是，可能存在主从延迟，如果主节点宕机，可能会丢数据。另外一种常用的策略是半同步复制，主节点等待操作日志最少成功复制到 N 个从节点上之后，再更新状 态，这种方式在性能、高可用和数据可靠性几个方面都比较平衡，很多分布式存储系统默认 采用的都是这种方式。

查找的时间复杂度又取 决于两个因素：

1. 查找算法；
2. 存储数据的数据结构。

MySQL 的 InnoDB 存储引擎，它的存储结构是 B+ 树，查找算法大多就是树的查找，查找 的时间复杂度就是 O(log n)，这些都是固定的。那我们唯一能改变的，就是数据总量了。

​      解决海量数据导致存储系统慢的问题，思想非常简单，就是一个“拆”字，把一大坨 数据拆分成 N 个小坨，学名叫“分片（Shard）。

存档历史订单数据提升查询性能：

当单表的订单数据太多，多到影响性能的时候，首选的 方案是，归档历史订单。

所谓归档，其实也是一种拆分数据的策略。简单地说，就是把大量的历史订单移到另外一张 历史订单表中。为什么这么做呢？因为像订单这类具有时间属性的数据，都存在热尾效应。 大多数情况下访问的都是最近的数据，但订单表里面大量的数据都是不怎么常用的老数据。

因为新数据只占数据总量中很少的一部分，所以把新老数据分开之后，新数据的数据量就会 少很多，查询速度也就会快很多。老数据虽然和之前比起来没少多少，查询速度提升不明 显，但是，因为老数据访很少会被访问到，所以慢一点儿也问题不大。

拆分的另外一个好处是，拆分订单时，需要改动的代码非常少。

如果你的数据库已经做了主从分离，那最好是去从库查询订单，再写到主库的 历史订单表中去，这样对主库的压力会小一点儿。

1. 首先我们需要创建一个和订单表结构一模一样的历史订单表；

2. 然后，把订单表中的历史订单数据分批查出来，插入到历史订单表中去。这个过程你怎 么实现都可以，用存储过程、写个脚本或者写个导数据的小程序都行，用你最熟悉的方 法就行。如果你的数据库已经做了主从分离，那最好是去从库查询订单，再写到主库的 历史订单表中去，这样对主库的压力会小一点儿。

3. 现在，订单表和历史订单表都有历史订单数据，先不要着急去删除订单表中的数据，你 应该测试和上线支持历史订单表的新版本代码。因为两个表都有历史订单，所以现在这 个数据库可以支持新旧两个版本的代码，如果新版本的代码有 Bug，你还可以立刻回滚 到旧版本，不至于影响线上业务。

4. 等新版本代码上线并验证无误之后，就可以删除订单表中的历史订单数据了。

5. 最后，还需要上线一个迁移数据的程序或者脚本，定期把过期的订单从订单表搬到历史 订单表中去。

   

迁移这么大量的数 据，或多或少都会影响数据库的性能，你应该尽量放在闲时去迁移，迁移之前一定做好备 份，这样如果不小心误操作了，也能用备份来恢复。

#### 12.如何批量删除大量数据

~~~sql
delete from orders where timestamp < SUBDATE(CURDATE(),INTERVAL 3 month);

delete from orders where timestamp < SUBDATE(CURDATE(),INTERVAL 3 month) order by id limit 1000;

select max(id) from orders where timestamp < SUBDATE(CURDATE(),INTERVAL 3 month);

delete from orders where id <= ? order by id limit 1000;
~~~



​        这样每次删除的时候，由于条件变成了主键比较，我们知道在 MySQL 的 InnoDB 存储引 擎中，表数据结构就是按照主键组织的一颗 B+ 树，而 B+ 树本身就是有序的，所以不仅查 找非常快，也不需要再进行额外的排序操作了。

​       为什么在删除语句中非得加一个排序呢？因为按 ID 排序后，我们每批 删除的记录，基本都是 ID 连续的一批记录，由于 B+ 树的有序性，这些 ID 相近的记录， 在磁盘的物理文件上，大致也是放在一起的，这样删除效率会比较高，也便于 MySQL 回 收页。

​     大量的历史订单数据删除完成之后，如果你检查一下 MySQL 占用的磁盘空间，你会发现它占用的磁盘空间并没有变小，这是什么原因呢？这也是和 InnoDB 的物理存储结构有关 系。

​      虽然逻辑上每个表是一颗 B+ 树，但是物理上，每条记录都是存放在磁盘文件中的，这些记 录通过一些位置指针来组织成一颗 B+ 树。当 MySQL 删除一条记录的时候，只能是找到记 录所在的文件中位置，然后把文件的这块区域标记为空闲，然后再修改 B+ 树中相关的一些 指针，完成删除。其实那条被删除的记录还是躺在那个文件的那个位置，所以并不会释放磁 盘空间。

​      这么做也是没有办法的办法，因为文件就是一段连续的二进制字节，类似于数组，它不支持 从文件中间删除一部分数据。如果非要这么删除，只能是把这个位置之后的所有数据往前 挪，这样等于是要移动大量数据，非常非常慢。所以，删除的时候，只能是标记一下，并不 真正删除，后续写入新数据的时候再重用这块儿空间。

磁盘空间足够的话，就这样吧，至少数据删了， 查询速度也快了，基本上是达到了目的。

​         如果说我们数据库的磁盘空间很紧张，非要把这部分磁盘空间释放出来，可以执行一次 OPTIMIZE TABLE 释放存储空间。对于 InnoDB 来说，执行 OPTIMIZE TABLE 实际上就 是把这个表重建一遍，执行过程中会一直锁表，也就是说这个时候下单都会被卡住，这个是需要注意的。另外，这么优化有个前提条件，MySQL 的配置必须是每个表独立一个表空间 （innodb_file_per_table = ON），如果所有表都是放在一起的，执行 OPTIMIZE TABLE 也不会释放磁盘空间。

alter table A engine=InnoDB 命令来重建这样也能达到释放空间的效果吧

重建表的过程中，索引也会重建，这样表数据和索引数据都会更紧凑，不仅占用磁盘空间更 小，查询效率也会有提升。那对于频繁插入删除大量数据的这种表，如果能接受锁表，定期 执行 OPTIMIZE TABLE 是非常有必要的。

系统可以接受暂时停服，最快的方法是这样的：直接新建一个临时订单表， 然后把当前订单复制到临时订单表中，再把旧的订单表改名，最后把临时订单表的表名改成 正式订单表。这样，相当于我们手工把订单表重建了一次，但是，不需要漫长的删除历史订 单的过程了。

~~~sql
-- 新建一个临时订单表
create table orders_temp like orders;
-- 把当前订单复制到临时订单表中
insert into orders_temp
select * from orders
where timestamp >= SUBDATE(CURDATE(),INTERVAL 3 month);
-- 修改替换表名
rename table orders to orders_to_be_droppd, orders_temp to orders;
-- 删除旧表
drop table orders_to_be_dropp
~~~



#### 13 分库分表

​    分库还是分表之前，我们需要先明确一个原则，那就是能不拆就不拆，能少拆 不多拆。原因也很简单，你把数据拆分得越散，开发和维护起来就越麻烦，系统出问题的概 率就越大。

只读的查询可以通过缓存和主从分离来解决，解决查询慢，只要减少每次查 询的数据总量就可以了，也就是说，分表就可以解决问题。

解决高并发的问题是需要分库的。 简单地说，数据量大，就分表；并发高，就分库。

#### 14 如何选择 Sharding Key

​    分库分表还有一个重要的问题是，选择一个合适的列或者说是属性，作为分表的依据，这个 属性一般称为 Sharding Key。

选择这个 Sharding Key 最重要的参考因素是，我们的业务是如何访问数据的。

一般的做法是，把订单数据同步到其他的存储系统中去，在其他的存储系统里面解决问题。 比如说，我们可以再构建一个以店铺 ID 作为 Sharding Key 的只读订单库，专门供商家来 使用。或者，把订单数据同步到 HDFS 中，然后用一些大数据技术来生成订单相关的报 表。

一旦做了分库分表，就会极大地限制数据库的查询能力，之前很简单的查询，分 库分表之后，可能就没法实现了。

分库分表一定是，数据量和并发大到所有 招数都不好使了，我们才拿出来的最后一招。

#### 15.如何选择分片算法

比如说，我分 12 个分片，每个月一个分片，这样对查询的兼容要好很多，毕竟查询条 件中带上时间范围，让查询只落到某一个分片上，还是比较容易的，我在查询界面上强制用 户必须指定时间范围就行了。 这种做法有个很大的问题，比如现在是 3 月份，那基本上所有的查询都集中在 3 月份这个 分片上，其他 11 个分片都闲着，这样不仅浪费资源，很可能你 3 月那个分片根本抗不住几 乎全部的并发请求。这个问题就是“热点问题”。

也就是说，我们希望并发请求和数据能均匀地分布到每一个分片上，尽量避免出现热点。这 是选择分片算法时需要考虑的一个重要的因素。

基于范围来分片容易产生热点问题，不适合作为订单的分片方法，但是这种分片方法的优点 也很突出，那就是对查询非常友好，基本上只要加上一个时间范围的查询条件，原来该怎么 查，分片之后还可以怎么查。范围分片特别适合那种数据量非常大，但并发访问量不大的 ToB 系统。比如说，电信运营商的监控系统，它可能要采集所有人手机的信号质量，然后 做一些分析，这个数据量非常大，但是这个系统的使用者是运营商的工作人员，并发量很 少。这种情况下就很适合范围分片。

一般来说，订单表都采用更均匀的哈希分片算法。比如说，我们要分 24 个分片，选定了 Sharding Key 是用户 ID，那我们决定某个用户的订单应该落到那个分片上的算法是，拿用 户 ID 除以 24，得到的余数就是分片号。这是最简单的取模算法，一般就可以满足大部分 要求了。当然也有一些更复杂的哈希算法，像一致性哈希之类的，特殊情况下也可以使用。

哈希分片算法能够分得足够均匀的前提条件是，用户 ID 后几位数字必 须是均匀分布的。比如说，你在生成用户 ID 的时候，自定义了一个用户 ID 的规则，最后 一位 0 是男性，1 是女性，这样的用户 ID 哈希出来可能就没那么均匀，可能会出现热点。

还有一种分片的方法：查表法。查表法其实就是没有分片算法，决定某个 Sharding Key 落 在哪个分片上，全靠人为来分配，分配的结果记录在一张表里面。每次执行查询的时候，先 去表里查一下要找的数据在哪个分片中。

查表法的好处就是灵活，怎么分都可以，你用上面两种分片算法都没法分均匀的情况下，就 可以用查表法，人为地来把数据分均匀了。查表法还有一个特好的地方是，它的分片是可以 随时改变的。比如我发现某个分片已经是热点了，那我可以把这个分片再拆成几个分片，或 者把这个分片的数据移到其他分片中去，然后修改一下分片映射表，就可以在线完成数据拆 分了。

分片映射表本身的数据不能太多，否则这个表反而成为热点和性能瓶颈 了。查表法相对其他两种分片算法来说，缺点是需要二次查询，实现起来更复杂，性能上也 稍微慢一些。但是，分片映射表可以通过缓存来加速查询，实际性能并不会慢很多。

常用三种分片算法，范围分片容易产生热点问题，但对查询更友好，适合适合并发量不 大的场景；哈希分片比较容易把数据和查询均匀地分布到所有分片中；查表法更灵活，但性 能稍差。

#### 16.Redis Cluster 如何解决数据量大、高可用和高并发问题？

Redis 从 3.0 版本开始，提供了官方的集群支持，也就是 Redis Cluser。Redis Cluster 相 比于单个节点的 Redis，能保存更多的数据，支持更多的并发，并且可以做到高可用，在 单个节点故障的情况下，继续提供服务。

和 MySQL 分库分表的方式类似，Redis Cluster 也是通过分片 的方式，把数据分布到集群的多个节点上。

Redis Cluster 是如何来分片的呢？它引入了一个“槽（Slot）”的概念，这个槽就是哈希 表中的哈希槽，槽是 Redis 分片的基本单位，每个槽里面包含一些 Key。每个集群的槽数 是固定的 16384（16 * 1024）个，每个 Key 落在哪个槽中也是固定的，计算方法是：

HASH_SLOT = CRC16(key) mod 16384

这些槽又是如何存放到具体的 Redis 节点上的呢？这个映射关系保存在集群的每个 Redis 节点上，集群初始化的时候，Redis 会自动平均分配这 16384 个槽，也可以通过命令来调 整。这个分槽的方法，分片算法：查表法。

客户端可以连接集群的任意一个节点来访问集群的数据，当客户端请求一个 Key 的时候， 被请求的那个 Redis 实例先通过上面的公式，计算出这个 Key 在哪个槽中，然后再查询槽 和节点的映射关系，找到数据所在的真正节点，如果这个节点正好是自己，那就直接执行命 令返回结果。如果数据不在当前这个节点上，那就给客户端返回一个重定向的命令，告诉客 户端，应该去连哪个节点上请求这个 Key 的数据。然后客户端会再连接正确的节点来访 问。 解决分片问题之后，Redis Cluster 就可以通过水平扩容来增加集群的存储容量，但是，每 次往集群增加节点的时候，需要从集群的那些老节点中，搬运一些槽到新节点，你可以手动 指定哪些槽迁移到新节点上，也可以利用官方提供的 redis-trib.rb脚本来自动重新分配 槽，自动迁移。

分片可以解决 Redis 保存海量数据的问题，并且客观上提升了 Redis 的并发能力和查询性 能。但是并不能解决高可用的问题，每个节点都保存了整个集群数据的一个子集，任何一个 节点宕机，都会导致这个宕机节点上的那部分数据无法访问。

#### 17. Redis Cluster 是怎么解决高可用问题的

增加从节点，做主从复制。Redis Cluster 支持给每个分片增加 一个或多个从节点，每个从节点在连接到主节点上之后，会先给主节点发送一个 SYNC 命 令，请求一次全量复制，也就是把主节点上全部的数据都复制到从节点上。全量复制完成之 后，进入同步阶段，主节点会把刚刚全量复制期间收到的命令，以及后续收到的命令持续地 转发给从节点。

因为 Redis 不支持事务，所以它的复制相比 MySQL 更简单，连 Binlog 都省了，直接就是 转发客户端发来的更新数据命令来实现主从同步。如果某个分片的主节点宕机了，集群中的 其他节点会在这个分片的从节点中选出一个新的节点作为主节点继续提供服务。新的主节点 选举出来后，集群中的所有节点都会感知到，这样，如果客户端的请求 Key 落在故障分片 上，就会被重定向到新的主节点上。

#### 18. Redis Cluster 是如何应对高并发的。

一般来说，Redis Cluster 进行了分片之后，每个分片都会承接一部分并发的请求，加上 Redis 本身单节点的性能就非常高，所以大部分情况下不需要再像 MySQL 那样做读写分离 来解决高并发的问题。默认情况下，集群的读写请求都是由主节点负责的，从节点只是起一 个热备的作用。当然了，Redis Cluster 也支持读写分离，在从节点上读取数据。

#### 19.为什么 Redis Cluster 不适合超大规模集群？

Redis Cluster 的优点是易于使用。分片、主从复制、弹性扩容这些功能都可以做到自动 化，通过简单的部署就可以获得一个大容量、高可靠、高可用的 Redis 集群，并且对于应 用来说，近乎于是透明的。

 所以，Redis Cluster 是非常适合构建中小规模 Redis 集群，这里的中小规模指的是，大 概几个到几十个节点这样规模的 Redis 集群。 

但是 Redis Cluster 不太适合构建超大规模集群，主要原因是，它采用了去中心化的设 计。刚刚我们讲了，Redis 的每个节点上，都保存了所有槽和节点的映射关系表，客户端可 以访问任意一个节点，再通过重定向命令，找到数据所在的那个节点。那你有没有想过一个 问题，这个映射关系表，它是如何更新的呢？比如说，集群加入了新节点，或者某个主节点 宕机了，新的主节点被选举出来，这些情况下，都需要更新集群每一个节点上的映射关系 表。

Redis Cluster 采用了一种去中心化的流言 (Gossip) 协议来传播集群配置的变化。

所谓流言，就是八卦。eg 我们上学的时候，班上谁和谁偷偷好上了，搞对象，那用不 了一天，全班同学都知道了。咋知道的？张三看见了，告诉李四，李四和王小二特别好，又 告诉了王小二，这样人传人，不久就传遍全班了。这个就是八卦协议的传播原理。

这个八卦协议它的好处是去中心化，传八卦不需要组织，吃瓜群众自发就传开了。这样部署 和维护就更简单，也能避免中心节点的单点故障。八卦协议的缺点就是传播速度慢，并且是 集群规模越大，传播的越慢。这个也很好理解，比如说，换成某两个特别出名的明星搞对 象，即使是全国人民都很八卦，但要想让全国每一个人都知道这个消息，还是需要很长的时 间。在集群规模太大的情况下，数据不同步的问题会被明显放大，还有一定的不确定性，如 果出现问题很难排查。

#### 20.如何用 Redis 构建超大规模集群？

一种是基于代理的方式，在客户端和 Redis 节点之间，还需要增加一层代理服务。这个代 理服务有三个作用。

第一个作用是，负责在客户端和 Redis 节点之间转发请求和响应。客户端只和代理服务打 交道，代理收到客户端的请求之后，再转发到对应的 Redis 节点上，节点返回的响应再经 由代理转发返回给客户端。

第二个作用是，负责监控集群中所有 Redis 节点状态，如果发现有问题节点，及时进行主 从切换。

第三个作用就是维护集群的元数据，这个元数据主要就是集群所有节点的主从信息，以及槽 和节点关系映射表。用 HAProxy+Keepalived 来代理 MySQL 请求的架构是类似的，只 是多了一个自动分片路由的功能而已。

像开源的 Redis 集群方案 twemproxy和Codis，都是这种架构的。

优点: 是对客户端透明，在客户端视角来看，整个集群和一个超大容量的单节 点 Redis 是一样的。并且，由于分片算法是代理服务控制的，扩容也比较方便，新节点加 入集群后，直接修改代理服务中的元数据就可以完成扩容。

缺点：增加了一层代理转发，每次数据访问的链路更长了，必然 会带来一定的性能损失。而且，代理服务本身又是集群的一个单点，当然，我们可以把代理 服务也做成一个集群来解决单点问题，那样集群就更复杂了。

另外一种方式是，不用这个代理服务，把代理服务的寻址功能前移到客户端中去。客户端在 发起请求之前，先去查询元数据，就可以知道要访问的是哪个分片和哪个节点，然后直连对 应的 Redis 节点访问数据。

当然，客户端不用每次都去查询元数据，因为这个元数据是不怎么变化的，客户端可以自己 缓存元数据，这样访问性能基本上和单机版的 Redis 是一样的。如果某个分片的主节点宕 机了，新的主节点被选举出来之后，更新元数据里面的信息。对集群的扩容操作也比较简 单，除了迁移数据的工作必须要做以外，更新一下元数据就可以了。

虽然说， 这个元数据服务仍然是一个单点，但是它的数据量不大，访问量也不大，相对就比 较容易实现。我们可以用 ZooKeeper、etcd 甚至 MySQL 都能满足要求。这个方案应该是 最适合超大规模 Redis 集群的方案了，在性能、弹性、高可用几方面表现都非常好，缺点 是整个架构比较复杂，客户端不能通用，需要开发定制化的 Redis 客户端，只有规模足够 大的企业才负担得起。

这几种集群方案对一些类似于“KEYS”这类的多 KEY 命 令，都没法做到百分百支持。原因很简单，数据被分片了之后，这种多 KEY 的命令很可能 需要跨多个分片查询。当你的系统从单个 Redis 库升级到集群时，可能需要考虑一下这方 面的兼容性问题。

#### 21.redis集群新增分片后，线上怎么实现好的平滑迁移？

如果用的是官方的Redis Cluster，可以用它提供的redis-trib.rb自动平滑迁移。 像Codis也提供了平滑迁移的管理工具。如果是自建的集群，相对就比较麻烦了。

阿里云有个 redis 方案，冷数据存磁盘，热数据放在内存。

#### 22.怎么做MySQL to Redis数据同步的



缓存穿透：超大规模系统的不能承受之痛

构建 Redis 集群，由于集群可以水平扩容，那只要集群足够大，理 论上支持海量并发也不是问题。但是，因为并发请求的数量这个基数太大了，即使有很小比 率的请求穿透缓存，打到数据库上请求的绝对数量仍然不小。加上大促期间的流量峰值，还 是存在缓存穿透引发雪崩的风险。

​        不让请求穿透缓存不就行了？反正现在存储 也便宜，只要你买得起足够多的服务器，Redis 集群的容量就是无限的。不如把全量的数据 都放在 Redis 集群里面，处理读请求的时候，干脆只读 Redis，不去读数据库。这样就完 全没有“缓存穿透”的风险了，实际上很多大厂它就是这么干的。

​        在 Redis 中缓存全量的数据，又引发了一个新的问题，那就是，如何来更新缓存中的数据 呢？因为我们取消了缓存穿透的机制，这种情况下，从缓存读到数据可以直接返回，如果没 读到数据，那就只能返回错误了！所以，当系统更新数据库的数据之后，必须及时去更新缓 存。

#### 23. 怎么保证 Redis 中的数据和数据库中的数据同步更 新？

​        对于像订单服务这类核心的业务，一个可行的方法是，我们启动一个更新订单缓存的服务， 接收订单变更的 MQ 消息，然后更新 Redis 中缓存的订单数据。

​        因为这类核心的业务数 据，使用方非常多，本来就需要发消息，增加一个消费订阅基本没什么成本，订单服务本身 也不需要做任何更改。

​       唯一需要担心的一个问题是，如果丢消息了怎么办？因为现在消息是缓存数据的唯一来源， 一旦出现丢消息，缓存里缺失的那条数据永远不会被补上。所以，必须保证整个消息链条的 可靠性，不过好在现在的 MQ 集群，比如像 Kafka 或者 RocketMQ，它都有高可用和高可 靠的保证机制，只要你正确配置好，是可以满足数据可靠性要求的。

 MQ消费的时候有自动重试机制，并且不建议这个地方加重试次数的限制。如果Redis故 障，就让同步卡在那儿，等Redis恢复之后，就可以继续同步，这样不会丢数据。

使用 Binlog 实时更新 Redis 缓存,数据更新服务只负责处理业务逻辑，更新 MySQL，完全不用管如何去更新缓存。负责更新 缓存的服务，把自己伪装成一个 MySQL 的从节点，从 MySQL 接收 Binlog，解析 Binlog 之后，可以得到实时的数据变更信息，然后根据这个变更信息去更新 Redis 缓存。

这种收 Binlog 更新缓存的方案，和刚刚我们讲到的，收 MQ 消息更新缓存的方案，其实 它们的实现思路是一样的，都是异步订阅实时数据变更信息，去更新 Redis。只不过，直接 读取 Binlog 这种方式，它的通用性更强。

​       缺点是，实现订单缓存更新服务有点儿复杂，毕竟不像收消息，拿到的直接 就是订单数据，解析 Binlog 还是挺麻烦的。 有很多开源的项目就提供了订阅和解析 MySQL Binlog 的功能，下面我们以比较常用的开 源项目Canal

Canal 模拟 MySQL 主从复制的交互协议，把自己伪装成一个 MySQL 的从节点，向 MySQL 主节点发送 dump 请求，MySQL 收到请求后，就会开始推送 Binlog 给 Canal， Canal 解析 Binlog 字节流之后，转换为便于读取的结构化数据，供下游程序订阅使用。

无论是用 MQ 还是 Canal 来异步更新缓存，对整个更新服务的数据可 靠性和实时性要求都比较高，数据丢失或者更新慢了，都会造成 Redis 中的数据与 MySQL 中数据不同步。在把这套方案应用到生产环境中去的时候，需要考虑一旦出现不同步问题时 的降级或补偿方案。

#### 24 分布式存储：你知道对象存储是如何保存图片文件

​     保存像图片、音视频这类大文件，最佳的选择就是对象存储。对象存储不仅有 很好的大文件读写性能，还可以通过水平扩展实现近乎无限的容量，并且可以兼顾服务高可用、数据高可靠这些特性。

对象存储是原生的分布式存储系统。

​          对象存储对外提供的服务，其实就是一个近乎无限容量的大文件 KV 存储，所以对象存储和 分布式文件系统之间，没有那么明确的界限。对象存储的内部，肯定有很多的存储节点，用 于保存这些大文件，这个就是数据节点的集群。

​       为了管理这些数据节点和节点中的文件，还需要一个存储系统保存集群的节点信 息、文件信息和它们的映射关系。这些为了管理集群而存储的数据，叫做元数据 (Metadata)。

​        元数据对于一个存储集群来说是非常重要的，所以保存元数据的存储系统必须也是一个集 群。但是元数据集群存储的数据量比较少，数据的变动不是很频繁，加之客户端或者网关都 会缓存一部分元数据，所以元数据集群对并发要求也不高。一般使用类似ZooKeeper或 者etcd这类分布式存储就可以满足要求。

​       存储集群为了对外提供访问服务，还需要一个网关集群，对外接收外部请求，对内访 问元数据和数据节点。网关集群中的每个节点不需要保存任何数据，都是无状态的节点。有 些对象存储没有网关，取而代之的是客户端，它们的功能和作用都是一样的。



#### 25. 对象是如何拆分和保存的

一般来说，对象存储中保存的 文件都是图片、视频这类大文件。在对象存储中，每一个大文件都会被拆成多个大小相等的 块儿（Block），拆分的方法很简单，就是把文件从头到尾按照固定的块儿大小，切成一块 儿一块儿，最后一块儿长度有可能不足一个块儿的大小，也按一块儿来处理。块儿的大小一 般配置为几十 KB 到几个 MB 左右。

把大对象文件拆分成块儿的目的有两个：

第一是为了提升读写性能，这些块儿可以分散到不同的数据节点上，这样就可以并行读 写。

第二是把文件分成大小相等块儿，便于维护管理。

​       对象被拆成块儿之后，还是太过于碎片化了，如果直接管理这些块儿，会导致元数据的数据 量会非常大，也没必要管理到这么细的粒度。所以一般都会再把块儿聚合一下，放到块儿的 容器里面。这里的“容器”就是存放一组块儿的逻辑单元。

容器内的块儿数大多 是固定的，所以容器的大小也是固定的。

分布式存储系统，主要由数据节点集群、元数据集群和网关集群（或者 客户端）三部分构成。数据节点集群负责保存对象数据，元数据集群负责保存集群的元数 据，网关集群和客户端对外提供简单的访问 API，对内访问元数据和数据节点读写数据。 为了便于维护和管理，大的对象被拆分为若干固定大小的块儿，块儿又被封装到容器（也就 分片）中，每个容器有一主 N 从多个副本，这些副本再被分散到集群的数据节点上保存。

所有分布式存储系统共通 的一些特性，对象存储也都具备，比如说数据如何分片，如何通过多副本保证数据可靠性， 如何在多个副本间复制数据，确保数据一致性等

#### 26.  如何保证数据同步的实时性

有些接收 Binlog 消息的下游业务，对 数据的实时性要求比较高，不能容忍太高的同步时延。

​       Canal 和 MQ 这两个环节，由于没什么业务逻辑，性能都非常好。所以，一般容易成为性能瓶颈的 就是消费 MQ 的同步程序，因为这些同步程序里面一般都会有一些业务逻辑，而且如果下 游的数据库写性能跟不上，表象也是这个同步程序处理性能上不来，消息积压在 MQ 里 面。

MySQL 主从同步 Binlog，是一个单线程的同步过程。为什么是单线程？原因 很简单，在从库执行 Binlog 的时候，必须按顺序执行，才能保证数据和主库是一样的。为 了确保数据一致性，Binlog 的顺序很重要，是绝对不能乱序的。

只要保证每个订单的更新操作日志的顺序别乱就可以了。这种一致性要求称 为因果一致性（Causal Consistency），有因果关系的数据之间必须要严格地保证顺序， 没有因果关系的数据之间的顺序是无所谓的。

首先根据下游同步程序的消费能力，计算出需要多少并发；然后设置 MQ 中主题的分区 （队列）数量和并发数一致。因为 MQ 是可以保证同一分区内，消息是不会乱序的，所以 我们需要把具有因果关系的 Binlog 都放到相同的分区中去，就可以保证同步数据的因果一 致性。对应到订单库就是，相同订单号的 Binlog 必须发到同一个分区上。

如果下游处理能力不能满足要求，可以增加 MQ 中的分区数量实现并发同步，但需要结合 同步的业务数据特点，把具有因果关系的数据哈希到相同分区上，才能避免因为并发乱序而 出现数据同步错误的问题。

#### 27 .如果预估了分区（队列）数量之后 随着业务数据的增长 需要增加分区 提高并发 怎么去做 扩容？ 因为统一笔订单需要打到同一个分区上

1. 停掉Canel； 2. 等MQ中所有的消息都消费完了。 3. 扩容MQ分区数，增加消费者实例数量。 4. 重新启动Canel。

#### 28.  如何在不停机的情况下，安全地更换数据库

墨菲定律：“如果事情有变坏的可能，不管这种可能性有多小，它总会发 生。”

我们在设计迁移方案的时候，一定要做到，每一步都是可逆的。要保证，每执行一个步 骤后，一旦出现问题，能快速地回滚到上一个步骤。

怎么来实现两个异构数据库之间的数据实时同步：

们可以 使用 Binlog 实时同步数据。如果源库不是 MySQL 的话，就麻烦一点儿，但也可以参考我 们讲过的，复制状态机理论来实现。这一步不需要回滚，原因是，只增加了一个新库和一个 同步程序，对系统的旧库和程序都没有任何改变。即使新上线的同步程序影响到了旧库，只 要停掉同步程序就可以了。

还需要改造一下订单服务，业务逻辑部分不需要变，DAO层需要做如下改造：

1 支持双写新旧两个库，并且预留热切换开关，能通过开关控制三种写状态：只写旧库、 只写新库和同步双写。 2. 支持读新旧两个库，同样预留热切换开关，控制读旧库还是新库。

然后上线新版的订单服务，这个时候订单服务仍然是只读写旧库，不读写新库。让这个新版 的订单服务需要稳定运行至少一到二周的时间，期间除了验证新版订单服务的稳定性以外， 还要验证新旧两个订单库中的数据是否是一致的。这个过程中，如果新版订单服务有问题， 可以立即下线新版订单服务，回滚到旧版本的订单服务。

稳定一段时间之后，就可以开启订单服务的双写开关了。开启双写开关的同时，需要停掉同 步程序。这里面有一个问题需要注意一下，就是这个双写的业务逻辑，一定是先写旧库，再 写新库，并且以写旧库的结果为准。

新库与旧库的数据可能会存在不一致的情况，原因有两个：一是停止同步 程序和开启双写，这两个过程很难做到无缝衔接，二是双写的策略也不保证新旧库强一致， 这时候我们需要上线一个对比和补偿的程序，这个程序对比旧库最近的数据变更，然后检查 新库中的数据是否一致，如果不一致，还要进行补偿。

像订单这类时效性强的数据，是比较好对比和补偿的。因为订单一旦完成之后，就几乎不会 再变了，那我们的对比和补偿程序，就可以依据订单完成时间，每次只对比这个时间窗口内 完成的订单。补偿的逻辑也很简单，发现不一致的情况后，直接用旧库的订单数据覆盖新库 的订单数据就可以了。 这样，切换双写期间，少量不一致的订单数据，等到订单完成之后，会被补偿程序修正。后 续只要不是双写的时候，新库频繁写入失败，就可以保证两个库的数据完全一致。

比如像商品信息这类数据，随时都有可能会变化。如果说数据 上有更新时间，那我们的对比程序可以利用这个更新时间，每次在旧库取一个更新时间窗口 内的数据，去新库上找相同主键的数据进行对比，发现数据不一致，还要对比一下更新时 间。如果新库数据的更新时间晚于旧库数据，那可能是对比期间数据发生了变化，这种情况 暂时不要补偿，放到下个时间窗口去继续对比。另外，时间窗口的结束时间，不要选取当前 时间，而是要比当前时间早一点儿，比如 1 分钟前，避免去对比正在写入的数据。 如果数据连时间戳也没有，那只能去旧库读取 Binlog，获取数据变化，然后去新库对比和 补偿。

1. 上线同步程序，从旧库中复制数据到新库中，并实时保持同步； 2. 上线双写订单服务，只读写旧库； 3. 开启双写，同时停止同步程序； 4. 开启对比和补偿程序，确保新旧数据库数据完全一样； 5. 逐步切量读请求到新库上； 6. 下线对比补偿程序，关闭双写，读写都切换到新库上； 7. 下线旧库和订单服务的双写功能。

#### 29.  类似“点击流”这样的海量数据应该如何存储

​       使用 Kafka 存储海量原始数据，早期对于这类海量原始数据，都倾向于先计算再存储。在接收原始数据的服务中， 先进行一些数据过滤、聚合等初步的计算，将数据先收敛一下，再落存储。这样可以降低存 储系统的写入压力，也能节省磁盘空间。

​        这几年，随着存储设备越来越便宜，并且，数据的价值被不断地重新挖掘，更多的大厂都倾 向于先存储再计算，直接保存海量的原始数据，再对数据进行实时或者批量计算。

几种常用的解决方案:

第一种：使用 Kafka 来存储。有的同学会问了，Kafka 不是一个消息队列么，怎么 成了存储系统了？那我告诉你，现代的消息队列，本质上就是分布式的流数据存储系统。

​       Kafka 提供“无限”的消息堆积能力，具有超高的吞吐量，可以满足我们保存原始数据的大 部分要求。写入点击流数据的时候，每个原始数据采集服务作为一个生产者，把数据发给 Kafka 就可以了。下游的计算任务，可以作为消费者订阅消息，也可以按照时间或者位点来 读取数据。并且，Kafka 作为事实标准，和大部分大数据生态圈的开源软件都有非常好的兼 容性和集成度，像 Flink、Spark 等大多计算平台都提供了直接接入 Kafka 的组件。

即使它支持扩容分片数量，也没办法像其他分布式存储系统那样，重新分配数据，把已有分 片上的数据迁移一部分到新的分片上。所以扩容分片也解决不了已有分片写满的问题。而 Kafka 又不支持按照时间维度去分片，所以，受制于单节点的存储容量，Kafka 实际能存 储的数据容量并不是无限的。

第二种方案是，使用 HDFS 来存储。使用 HDFS 存储数据也很简单，就是把原始数据写成 一个一个文本文件，保存到 HDFS 中。我们需要按照时间和业务属性来组织目录结构和文 件名，以便于下游计算程序来读取，比如说：“click/20200808/Beijing_0001.csv”， 代表 2020 年 8 月 8 日，从北京地区用户收集到的点击流数据，这个是当天的第一个文 件。 对于保存海量的原始数据这个特定的场景来说，HDFS 的吞吐量是远不如 Kafka 的。按照 平均到每个节点上计算，Kafka 的吞吐能力很容易达到每秒钟大几百兆，而 HDFS 只能达 到百兆左右。这就意味着，要达到相同的吞吐能力，使用 HDFS 就要比使用 Kafka，多用 几倍的服务器数量。 但 HDFS 也有它的优势，第一个优势就是，它能提供真正无限的存储容量，如果存储空间 不够了，水平扩容就可以解决。另外一个优势是，HDFS 能提供比 Kafka 更强的数据查询 能力。Kafka 只能按照时间或者位点来提取数据，而 HDFS 配合 Hive 直接就可以支持用 SQL 对数据进行查询，虽然说查询的性能比较差，但查询能力要比 Kafka 强大太多了。 以上这两种方案因为都有各自的优势和不足，在实际生产中，都有不少的应用，你可以根据 业务的情况来选择。



既有超高的吞吐能 力，又能无限扩容，同时还能提供更好的查询能力，有这样的好事儿么？

一类是分布式流数据存储，比较活跃的项目有Pravega和 Pulsar 的存储引擎Apache BookKeeper。我所在的团队也在这个方向上持续探索中，也开源了我们的流数据存储项目 JournalKeeper

有一类是时序数据库（Time Series Databases），比较活跃的项目有InfluxDB和 OpenTSDB等。这些时序数据库，不仅有非常好的读写性能，还提供很方便的查询和聚 合数据的能力。但是，它们不是什么数据都可以存的，它们专注于类似监控数据这样，有时 间特征并且数据内容都是数值的数据。如果你有存储海量监控数据的需求，

#### 30 .面对海量数据，如何才能查得更快

一般用于分析的数据量都会比在线业务大出几个数量级，这需要存储系统能保存海量数 据； 1. 能在海量的数据上做快速的聚合、分析和查询。注意这里面所说的“快速”，前提是处 理 GB、TB 甚至 PB 级别的海量数据，在这么大的数据量上做分析，几十秒甚至几分钟 都算很快了，和在线业务要求的毫秒级速度是不一样的； 3. 由于数据大多数情况下都是异步写入，对于写入性能和响应时延，一般要求不高； 4. 分析类系统不直接支撑前端业务，所以也不要求高并发。



如果你的系统的数据量在 GB 量级以下，MySQL 仍然是可以考虑的，因为它的查询能力足以应付大部分分析系统的业务需求。并且可以和在 线业务系统合用一个数据库，不用做 ETL（数据抽取），省事儿并且实时性好。这里还是要 提醒你，最好给分析系统配置单独的 MySQL 实例，避免影响线上业务。

数据量级已经超过 MySQL 极限，可以选择一些列式数据库，比如：HBase、 Cassandra、ClickHouse，这些产品对海量数据，都有非常好的查询性能，在正确使用的 前提下，10GB 量级的数据查询基本上可以做到秒级返回。高性能的代价是功能上的缩水， 这些数据库对数据的组织方式都有一些限制，查询方式上也没有 MySQL 那么灵活。大



ES 本来是一个为了搜索而生的存储产 品，但是也支持结构化数据的存储和查询。由于它的数据都存储在内存中，并且也支持类似 于 Map-Reduce 方式的分布式并行查询，所以对海量结构化数据的查询性能也非常好。 最重要的是，ES 对数据组织方式和查询方式的限制，没有其他列式数据库那么死板。也就 是说，ES 的查询能力和灵活性是要强于上述这些列式数据库的。但是 ES 有一个缺点，就是你需要给它准备大内存的 服务器，硬件成本有点儿高。

数据量级超过 TB 级的时候，对这么大量级的数据做统计分析，无论使用什么存储系统，都 快不到哪儿去。这个时候的性能瓶颈已经是磁盘 IO 和网络带宽了。这种情况下，实时的查 询和分析肯定做不了。

解决的办法都是，定期把数据聚合和计算好，然后把结果保存起来， 在需要时对结果再进行二次查询。这么大量级的数据，一般都选择保存在 HDFS 中，配合 Map-Reduce、Spark、Hive 等等这些大数据生态圈产品做数据聚合和计算。

#### 31.转变你的思想：根据查询来选择存储系统

存储系统没有银弹，不要指望简单地更换一种数据库，就可以解决数据量大，查询慢 的问题。

但是，在特定的场景下，通过一些优化方法，把查询性能提升几十倍甚至几百倍，这个都是 有可能的。这里面有个很重要的思想就是，根据查询来选择存储系统和数据结构。

海量数据的主要用途就是支撑离线分析类业务的查询，根据数据量规模不同，由小到大可以 选择：关系型数据库，列式数据库和一些大数据存储系统。对于 TB 量级以下的数据，如果 可以接受相对比较贵的硬件成本，ES 是一个不错的选择。

 对于海量数据来说，选择存储系统没有银弹，重要的是转变思想，根据业务对数据的查询方 式，反推数据应该使用什么存储系统、如何分片，以及如何组织。即使是同样一份数据，也 要根据不同的查询需求，组织成不同的数据结构，存放在适合的存储系统中，才能在每一种 业务中都达到理想的查询性能。

#### 32. NewSQL是如何解决高可用、分片问题的

NoSQL ：牺牲了 SQL 这种强大的查询能力和 ACID 事务支持。

完整地支持 SQL 和 ACID，提供和 Old SQL 隔离级别相当的事务能力；

 高性能、高可靠、高可用，支持水平扩容。

像 Google 的 Cloud Spanner、国产的 OceanBase 以及开源的CockroachDB都属于 New SQL 数据库。Cockroach 这个英文单词是蟑螂的意思，所以一般我们都把 CockroachDB 俗称为小强数据库。



MySQL 的存储引擎 InnoDB，实际上是基于文件系统的 B+ 树，像 Hive 和 HBase，它们的存储引擎都是基于 HDFS 构建的。

它的分片算法采用的是范围分片，我们之前也讲到过，范围分片对查询是最友好的，可以很 好地支持范围扫描这一类的操作，这样有利于它支撑上层的 SQL 查询。

它采用 Raft一致性协议来实现每个分片的高可靠、高可用和强一致。这个 Raft 协议，它 的一个理论基础，就是我们之前讲的复制状态机，并且在复制状态机的基础上，Raft 实现 了集群自我监控和自我选举来解决高可用的问题。Raft 也是一个被广泛采用的、非常成熟 的一致性协议，比如 etcd 也是基于 Raft 来实现的。



CockroachDB 的元数据直接分布在所有的存储节点上，依靠流言协议来传播

Redis Cluster 中也是用流言协议来传播元数据变化的。

CockroachDB 用上面这些成熟的技术解决了集群问题，在单机的存储引擎上，更是直接使 用了 RocksDB 作为它的 KV 存储引擎。

CockroachDB 能提供金融级的事务隔离性么？

CockroachDB 提供了另外两种隔离级别，分别是：Snapshot Isolation (SI) 和 Serializable Snapshot Isolation (SSI)，其中 SSI 是 CockroachDB 默认的隔离级别。

#### 33. RocksDB：不丢数据的高性能KV存储



RocksDB是 Facebook 开源的一个高性能持久化 KV 存储。目前，你可能很少见到过哪 个项目会直接使用 RocksDB 来保存数据，在未来，RocksDB 大概率也不会像 Redis 那样 被业务系统直接使用。

那我们为什么要关注它呢？ 因为越来越多的新生代数据库，都不约而同地选择 RocksDB 作为它们的存储引擎。在将 来，很有可能出现什么样的情况呢？我们使用的很多不同的数据库，它们背后采用的存储引 擎都是 RocksDB。



RocksDB 采用了一个非常复杂的数据存储结构，并且这个存储结构采用了内存和磁盘混合 存储方式，使用磁盘来保证数据的可靠存储，并且利用速度更快的内存来提升读写性能。或 者说，RocksDB 的存储结构本身就自带了内存缓存。

RocksDB 它的数据结构，可以让 绝大多数写入磁盘的操作都是顺序写。那我们知道，无论是 SSD 还是 HDD 顺序写的性能 都要远远好于随机写，这就是 RocksDB 能够做到高性能写入的根本原因。

Kafka 也是采用顺序读写的方式，所以它的读写性能也是超级快。但是这种顺序写入的数据 基本上是没法查询的，因为数据没有结构，想要查询的话，只能去遍历。

RocksDB 究竟使 用了什么样的数据结构，在保证数据顺序写入的前提下还能兼顾很好的查询性能呢？这种数 据结构就是 LSM-Tree。

LSM-Tree 的全称是：The Log-Structured Merge-Tree，是一种非常复杂的复合数据结 构，它包含了 WAL（Write Ahead Log）、跳表（SkipList）和一个分层的有序表 （SSTable，Sorted String Table）。

RocksDB 是一个高性能持久化的 KV 存储，被很多新生代的数据库作为存储引擎。 RocksDB 在保证不错的读性能的前提下，大幅地提升了写入性能，这主要得益于它的数据 结构：LSM-Tree。 LSM-Tree 通过混合内存和磁盘内的多种数据结构，将随机写转换为顺序写来提升写性能， 通过异步向下合并分层 SSTable 文件的方式，让热数据的查找更高效，从而获得还不错的 综合查找性能。 通过分析 LSM-Tree 的数据结构可以看出来，这种数据结构还是偏向于写入性能的优化， 更适合在线交易类场景，因为在这类场景下，需要频繁写入数据。





